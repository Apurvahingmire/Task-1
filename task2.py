# -*- coding: utf-8 -*-
"""task2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14qnZP-0WDN5BVnkp0MLFdFIQQkGCT62W

<p>
<font style="times new roman">NAME: Apurva Charankumar Hingmire <br>

TSF Internship Data Science and Business Analytics <br>

<b>TASK 2(beginner): Prediction using unsupervised ML-</b> From the given 'Iris' dataset, predicting the optimum number of clusters and representing it visually using unsupervised ML. <br><br>
Language: Python <br>
IDE: Google colab
</font>
<p>
<hr>

<p>
<font size="4"><b>Step1:  Importing required libraries</b></font>
</p>
"""

import numpy as np 
import matplotlib.pyplot as plt 
import pandas as pd
import seaborn as sns  
from sklearn import datasets

"""<p>
<font size="4"><b>Step2: Loading the dataset</b></font>
</p>
"""

url='http://bit.ly/3KXTdox'
iris = datasets.load_iris()
iris_df.head(12)

iris_df = pd.DataFrame(iris.data, columns = iris.feature_names)
iris_df.head()

iris_df.tail()

iris_df.shape

"""<p>
<font size="3"> Checking for Null and Categorical Data</font>
</p>
"""

iris_df.isnull().sum()

iris_df.info()

iris_df.describe()

"""<p>
<font size= "4"><b>Step3: visualizing the Dataset</b></font>
</p>
"""

#plotting the pairwise plot of multiple features(variables) in a grid format
sns.pairplot(data=iris_df)

"""<p>
<font size="4"><b> Step4: Clustering using k-means
"""

# Finding the optimum number of clusters for k-means classification

x = iris_df.iloc[:, [0, 1, 2, 3]].values

from sklearn.cluster import KMeans
wcss = []               #Array that will store 'within cluster sum of squares' for all 10 values of k


#We are taking number of clusters ranging from 1 to 10
for i in range(1, 11):
     kmeans = KMeans(n_clusters = i, init = 'k-means++', 
                     max_iter = 300, n_init = 10, random_state = 0)
     kmeans.fit(x)
     wcss.append(kmeans.inertia_)

#To find optimum k value, we are using elbow method. Plotting line graph to observe the elbow

plt.plot(range(1, 11), wcss, marker ='o') 
plt.title('The elbow method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')   # within cluster sumof squares
plt.show()

"""<p>
<font size="4"><b> Model Training</b></font>
</p>

"""

#Applying kmeans for k=3
kmeans = KMeans(n_clusters = 3, init = 'k-means++',
                max_iter = 300, n_init = 10, random_state = 0)
y_kmeans = kmeans.fit_predict(x)

"""<font size="4"><b>Step5: Cluster Visualization of Sepal llength and its width</b></font>"""

# Visualizing the clusters on first 2 columns
plt.figure()
plt.scatter(x[y_kmeans == 0, 0], x[y_kmeans == 0, 1],
            c = 'red', label = 'Iris-setosa')
plt.scatter(x[y_kmeans == 1, 0], x[y_kmeans == 1, 1],
            c = 'blue', label = 'Iris-versicolour')
plt.scatter(x[y_kmeans == 2, 0], x[y_kmeans == 2, 1],
            c = 'black', label ='Iris-virginica')

#Plotting the centroids of the clusters
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],
            c ='yellow', label = 'Centroids')

plt.legend(loc= 'upper right')
plt.xlabel('Sepal length')
plt.ylabel('Sepal width')
plt.show()

"""<font size="4"><b>Similarly, cluster visualization of Petal length and its width"""

plt.figure()
plt.scatter(x[y_kmeans == 0, 2], x[y_kmeans == 0, 3],
            c = 'red', label = 'Iris-setosa')
plt.scatter(x[y_kmeans == 1, 2], x[y_kmeans == 1, 3],
            c = 'blue', label = 'Iris-versicolour')
plt.scatter(x[y_kmeans == 2, 2], x[y_kmeans == 2, 3],
            c = 'black', label = 'Iris-virginica')

# Plotting the centroids of the clusters
plt.scatter(kmeans.cluster_centers_[:, 2], kmeans.cluster_centers_[:, 3],
            c = 'yellow', label = 'centroids')
plt.legend()
plt.xlabel('Petal length')
plt.ylabel('Petal width')
plt.show()

"""<font size="4"><b>Conclusion: The predicted number of clusters are equal to the number of clusters found by visualizing the same data, that is 3.

<font size="5"><b>THANK YOU!!</font></b>
"""